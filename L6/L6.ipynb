{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "# 6A - L1 Introduction to Motion\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Motion\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L61.gif\" />\n",
    "    <center><figcaption>Fig.1(a): example of gif</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "## Video \n",
    "\n",
    "A video is a sequene of frames captured over time - usually quickly\n",
    "\n",
    "- Now our image data is a function of smape (x,y) and time (t)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L62.png\" />\n",
    "    <center><figcaption>Fig.1(b)</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Applications: Bideo segmentation\n",
    "\n",
    "**Background subtraction**\n",
    "\n",
    "- A static camera is observing a scene\n",
    "- Goal: seperate the static background from the moving foreground\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L63.png\" />\n",
    "    <center><figcaption>Fig.2(a)</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Shot boundary detection**\n",
    "\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L64.png\" />\n",
    "    <center><figcaption>Fig.2(b)</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "**Motion segmentation**\n",
    "- Segment the video into multiple coherently moving objects\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L65.png\" />\n",
    "    <center><figcaption>Fig.2(c)</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion and perceptual organization\n",
    "\n",
    "- Gestalt psychology (Wertheimer, 1880-1943)\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L66.png\" />\n",
    "    <center><figcaption>Fig.3(a)</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- Sometimes, motion is the only cue\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L67.png\" width=\"200px\" />\n",
    "    <img src=\"imgs/L68.png\" width=\"200px\"/>\n",
    "    <img src=\"imgs/L69.gif\" width=\"200px\"/>\n",
    "    <center><figcaption>Fig.3(b)</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impverished Motion\n",
    "\n",
    "- Even \"impoverished\" motion data can evoke a strong percept\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L610.png\" width=\"200px\"/>\n",
    "    <img src=\"imgs/L611.gif\" width=\"200px\"/>\n",
    "    <center><figcaption>Fig.4: notice the center point is not moving. Focus on it and you see how weird it is. Try to take it out of your head after that !!!</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "Examples of Impoverished Motion\n",
    "\n",
    "Video: <a href=\"https://www.youtube.com/watch?v=1F5ICP9SYLU\">2-Dimensional Motion Perception</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Applications of Motion Analysis\n",
    "\n",
    "### Mosaicing\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L612.png\" width=\"400px\" />\n",
    "    <img src=\"imgs/L613.png\" width=\"400px\"/>\n",
    "    <center><figcaption>Fig.5</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "- **Segmentation of objects in space or time**\n",
    "- **Estimating 3D structure**\n",
    "- **Learning dynamical models - how things move**\n",
    "- **Recognizing events and activities**\n",
    "- **Improving video quality (motion stabilization)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Estimation Techniques\n",
    "\n",
    "**Featured-based methods**\n",
    "\n",
    "- Extract visual features (corners, textured areas) and track them over multiple frames\n",
    "- Sparse motion fields, but more robust tracking\n",
    "- Suitable when image motion is large (10s of pixels)\n",
    "\n",
    "**Direct, dense methods**\n",
    "\n",
    "- Directly recover image motion at each pixel from spatio-temporal image brightness variations\n",
    "- Dense motion fields, but sensitive to appearance variations\n",
    "- Suitable for video and whn image motion is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "# 6B - L1 Dense flow: Brightness constraints\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion estimation: Optical flow\n",
    "\n",
    "Optic flow is the **apparent** motion of objects or surfaces\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L614.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.6</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition: Optical Flow\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L615.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.7</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "How to estimate pixel motion from image $I(x,y,t)$ to $I(x,y,t+1)$?\n",
    "\n",
    "\n",
    "-> Solve pixel correspondence problem <br/>\n",
    "- Given a pixel in $I(x,y,t)$, look for *nearby* pixels of the *same color *in $I(x,y,t+1)$\n",
    "\n",
    "**This is the optic flow problem**\n",
    "\n",
    "\n",
    "- Color constancy: a point in $I(x,y,t)$ looks the same in $I(x',t',t+1)$ \n",
    "    - For grayscale images, this is the ***brightness constancy***\n",
    "- Small motion: points do not move very far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical flow constraints (grayscale images)\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L616.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.8</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "1) Brightness constancy constraint (equation)\n",
    "$$I(x,y,t) = I(x+u,y+v,t+1)$$\n",
    "$$0 = I(x+u,y+v,t+1)-I(x,y,t)$$\n",
    "\n",
    "2) Small motion: (u and v are less than 1 pixel, or smooth)\n",
    "\n",
    "Taylor series expansion of $I$:\n",
    "\n",
    "$$I(x+u,y+v) = I(x,y) + \\frac{\\partial I}{\\partial x}u + \\frac{\\partial I}{\\partial y}v + [higher\\, order\\, terms]$$\n",
    "$$I(x+u,y+v) \\approx I(x,y) + \\frac{\\partial I}{\\partial x}u + \\frac{\\partial I}{\\partial y}v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining these two equations:\n",
    "\n",
    "$$0 = I(x+u,y+v,t+1)-I(x,y,t)$$\n",
    "$$0 \\approx I(x,y,t+1)-I_xu + I_yv - I(x,y,t)$$\n",
    "\n",
    "$I_x$: = $\\frac{\\partial I}{\\partial x}$ for $t$ or $t+1$\n",
    "\n",
    "$$0 \\approx [I(x,y,t+1)- I(x,y,t)]+I_xu + I_yv $$\n",
    "$$0 \\approx I_t+I_xu + I_yv $$\n",
    "\n",
    "<font color=\"blue\">**$$I_t + \\nabla I \\cdot <u,v>$$**</font>\n",
    "<font color=\"blue\">**$$\\implies 0 \\approx I_t + \\nabla I \\cdot <u,v>$$**</font>\n",
    "In the limit as $u$ and $v$ approaches zero, this becomes exact:\n",
    "<font color=\"blue\">**$$0 = I_t + \\nabla I \\cdot <u,v>$$**</font>\n",
    "\n",
    "Brightness constancy constraint equation\n",
    "\n",
    "\n",
    "<font color=\"blue\">$$I_xu + I_yv + I_t = 0$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Component of Flow\n",
    "\n",
    "Q: How many unknowns and equations per pixel?\n",
    "\n",
    "<pre><font color=\"red\">2 unknown(u,v) but 1 equaton</font></pre>\n",
    "\n",
    "Intuitively, what does this constraint mean?\n",
    "\n",
    "- The component of the flow in the gradient direction is determined\n",
    "- The component of the flow parallel to an edge is unknown\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L617.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.9</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture problem\n",
    "\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L618.gif\" width=\"400px\" />\n",
    "    <img src=\"imgs/L619.gif\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.10 (a)</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "## Gradient component of flow\n",
    "\n",
    "Some falks say: \"This explains the Barber Pole illusion\"\n",
    "\n",
    "http://www.sandlotscience.com/Ambiguous/Barberpole_Illusion.htm <br/>\n",
    "http://www.liv.ac.uk/~marcob/Trieste/barberpole.html\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"imgs/L620.gif\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.10(b): This animation simulates a set of black and white stripes moving rightwards behind a black mask with four holes. Look at the leftmost hole and compare it with the bottom center hole. These are instances of the \"barberpole\" effect (Wallach,1935/1996). The perceived direction of motion is the predominant direction of the stripe terminators, following the longest side of the hole. In the rightmost hole, the sides are first horizontal, then vertical, then turn horizontal again. Accordingly, the stripes appear to go right, turn downwards, and then turn right again. In a circular hole, the terminators do not have a predominant direction and the stripes appear to move perpendicularly to their orientation. The barberpole effect suggests that contour terminators play an important role in determining the perceived motion of an object</figcaption></center>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additinal Flow Constraints Quize\n",
    "\n",
    "\n",
    "What additional constraints you can use: <br/>\n",
    "\n",
    "- <font color=\"green\">Nearby pixels move together</font>\n",
    "- <font color=\"green\">Motion must be consistent over the entire image</font>\n",
    "- <font color=\"green\">Only consider distinct regions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth Optical Flow (Horn and Schunck - long ago)\n",
    "\n",
    "- Formulate Error in Optical Flow Constraint:\n",
    "\n",
    "$$e_c = \\int \\int_{image} (I_xu+I_yv+I_t)^2dxdy$$\n",
    "\n",
    "- We need additional constraints (pardon the integrals)\n",
    "\n",
    "- Smoothness constraint: motion field tends to vary smoothly over the image\n",
    "\n",
    "$$e_s= \\int \\int_{image} (u_x^2 + u_y^2)+(v_x^2 + v_y^2)dxdy$$\n",
    "\n",
    "- Penalized for changes in $u$ and $v$ over the image\n",
    "\n",
    "Given both terms, find $(u,v)$ at each image point that minimizes:\n",
    "\n",
    "$$e=e_s+\\lambda e_c$$\n",
    "\n",
    "$\\lambda$: weighting factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">I kind of looked like Megan as I was typing the notes for this lesson (recording with no interest and no idea whatsoever of what Aaron is talking about). Let's hope the king charges me back with stormlight power and excitement as Barcelona faces Madrid in the copa del rey tomorrow</font>\n",
    "<img src=\"imgs/messi.png\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================================\n",
    "# 6B - L2 Dense flow: Lucas and Kanade\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Aperture Problem\n",
    "\n",
    "- Basic idea: Impose local constraints to get more equations for a pixel: \n",
    "    - E.g. assume that the flow field is smooth locally\n",
    "\n",
    "- One method: pretend the pixel's neighbors have the **same** (u,v) \n",
    "    - If we use a 5X5 window, that gives us 25 equations per pixel!\n",
    "\n",
    "\n",
    "$$0 = I_t(p_i)+\\nabla I(p_i)\\cdot[u\\,v]$$\n",
    "<br/>\n",
    "$$A_{25\\times2} \\times d_{2\\times1} = b_{25\\times1}$$\n",
    "<br/>\n",
    "$$\\begin{bmatrix}I_x(p_1) & I_y(p_1)\\\\I_x(p_2) & I_y(p_2)\\\\.&.\\\\.&.\\\\.&.\\\\I_x(p_25) & I_y(p_25)\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix} = - \\begin{bmatrix}I_t(p_1)\\\\I_t(p_2)\\\\.\\\\.\\\\.\\\\I_t(p_25)\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "### Lukas-Kanade flow\n",
    "\n",
    "**Problem**: We have more equations than unknowns <br/>\n",
    "$(d = [u\\, v])$ <br/>\n",
    "\n",
    "**Solution**: Least squares problem\n",
    "\n",
    "The summations are over all pixels in the $K \\times K$ window\n",
    "\n",
    "$$A_{25\\times2} \\times d_{2\\times1} = b_{25\\times1} \\rightarrow minimize ||Ad -b||^2$$\n",
    "\n",
    "$$(A^TA)_{2\\times2} d_{2\\times1} = A^Tb_{2\\times1}$$\n",
    "\n",
    "$$\\begin{bmatrix}\\sum I_xI_x&\\sum I_xI_y\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix} = - \\begin{bmatrix}\\sum I_xI_t \\\\ \\sum I_yI_t\\end{bmatrix}$$\n",
    "\n",
    "## Aperture Problem and Normal Flow\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L621.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.11 </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "The gradient constraints\n",
    "\n",
    "$$I_xu + I_yv + I_t = 0$$\n",
    "$$\\nabla I \\cdot \\vec{U} + I_t = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Local Constraints\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L622.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.12 </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "<font color=\"red\">$$\\nabla I^1 \\cdot U = -I_t^1$$</font>\n",
    "<font color=\"pink\">$$\\nabla I^2 \\cdot U = -I_t^2$$</font>\n",
    "<font color=\"green\">$$\\nabla I^3 \\cdot U = -I_t^3$$</font>\n",
    "<center>etc.</center>\n",
    "\n",
    "When is This Solvable?\n",
    "\n",
    "$$\\begin{bmatrix}\\sum I_xI_x&\\sum I_xI_y\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix} = - \\begin{bmatrix}\\sum I_xI_t \\\\ \\sum I_yI_t\\end{bmatrix}$$\n",
    "\n",
    "- $A^TA$ should be invertible\n",
    "- => So $A^TA$ should be well-conditioned $-\\frac{\\lambda_1}{\\lambda_2}$ should not be too large ($\\lambda_1 $= larger eigenvalue)\n",
    "\n",
    "### Eigenvectors of $A^TA$\n",
    "\n",
    "$$A^TA = \\begin{bmatrix}\\sum I_xI_x&\\sum I_xI_y\\\\\\sum I_xI_y&\\sum I_yI_y\\end{bmatrix} = \\sum \\begin{bmatrix}I_x\\\\I_y\\end{bmatrix} \\begin{bmatrix}I_x&I_y\\end{bmatrix} = \\sum \\nabla I(\\nabla I)^T$$\n",
    "\n",
    "- Also $A^TA$ should be solvable when there is no aperture problem\n",
    "    - Does this remind you of something???\n",
    "        - Recall the Harris corner detector:\n",
    "            - $M = A^TA$ is the second moment matrix\n",
    "        - The eigenvectors and eigenvalues of M relate to edge direction and magnitued\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output, Image as NoteImage, display\n",
    "def imshow(im,fmt='jpeg'):\n",
    "    #a = np.uint8(np.clip(im, 0, 255))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(im).save(f, fmt)\n",
    "    display(NoteImage(data=f.getvalue()))\n",
    "def imread(filename):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "def red(im):\n",
    "    return im[:,:,0]\n",
    "def green(im):\n",
    "    return im[:,:,1]\n",
    "def blue(im):\n",
    "    return im[:,:,2]\n",
    "def gray(im):\n",
    "    return cv2.cvtColor(im.copy(), cv2.COLOR_BGR2GRAY)\n",
    "def square(img,center,size,color=(0,255,0)):\n",
    "    y,x = center\n",
    "    leftUpCorner = (x-size,y-size)\n",
    "    rightDownCorner = (x+size,y+size)\n",
    "    cv2.rectangle(img,leftUpCorner,rightDownCorner,color,3)\n",
    "def normalize_img(s):\n",
    "    start = 0\n",
    "    end = 255\n",
    "    width = end - start\n",
    "    res = (s - s.min())/(s.max() - s.min()) * width + start\n",
    "    return res.astype(np.uint8)\n",
    "def line(img,x):\n",
    "    cv2.line(img,(0,x),(img.shape[1],x),(255,0,0),3) \n",
    "def mse(imageA, imageB):\n",
    "        # the 'Mean Squared Error' between the two images is the\n",
    "        # sum of the squared difference between the two images;\n",
    "        # NOTE: the two images must have the same dimension\n",
    "        err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "        err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "        # return the MSE, the lower the error, the more \"similar\"\n",
    "        # the two images are\n",
    "        return err\n",
    "def random_color():\n",
    "    color = list(np.random.choice(range(256), size=3))\n",
    "    return (int(color[0]),int(color[1]),int(color[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://sandipanweb.wordpress.com/2018/02/25/implementing-lucas-kanade-optical-flow-algorithm-in-python/\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "def optical_flow(I1g, I2g, window_size, tau=1e-2):\n",
    "\n",
    "    kernel_x = np.array([[-1., 1.], [-1., 1.]])\n",
    "    kernel_y = np.array([[-1., -1.], [1., 1.]])\n",
    "    kernel_t = np.array([[1., 1.], [1., 1.]])#*.25\n",
    "    w = window_size//2 # window_size is odd, all the pixels with offset in between [-w, w] are inside the window\n",
    "    I1g = I1g / 255. # normalize pixels\n",
    "    I2g = I2g / 255. # normalize pixels\n",
    "    # Implement Lucas Kanade\n",
    "    # for each point, calculate I_x, I_y, I_t\n",
    "    mode = 'same'\n",
    "    fx = signal.convolve2d(I1g, kernel_x, boundary='symm', mode=mode)\n",
    "    fy = signal.convolve2d(I1g, kernel_y, boundary='symm', mode=mode)\n",
    "    ft = signal.convolve2d(I2g, kernel_t, boundary='symm', mode=mode) + signal.convolve2d(I1g, -kernel_t, boundary='symm', mode=mode)\n",
    "    u = np.zeros(I1g.shape)\n",
    "    v = np.zeros(I1g.shape)\n",
    "    # within window window_size * window_size\n",
    "    for i in range(w, I1g.shape[0]-w):\n",
    "        for j in range(w, I1g.shape[1]-w):\n",
    "            Ix = fx[i-w:i+w+1, j-w:j+w+1].flatten()\n",
    "            Iy = fy[i-w:i+w+1, j-w:j+w+1].flatten()\n",
    "            It = ft[i-w:i+w+1, j-w:j+w+1].flatten()\n",
    "            \n",
    "            b = np.reshape(It, (It.shape[0],1)) # get b here\n",
    "            A = np.vstack((Ix, Iy)).T # get A here\n",
    "\n",
    "            if np.min(abs(np.linalg.eigvals(np.matmul(A.T, A)))) >= tau:\n",
    "                nu = np.matmul(np.linalg.pinv(A), b) # get velocity here\n",
    "                u[i,j]=nu[0]\n",
    "                v[i,j]=nu[1]\n",
    "    return (u,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import imageio\n",
    "def decode_gif_file(fname):\n",
    "    imgs = []\n",
    "    gif = imageio.mimread(fname)\n",
    "    nums = len(gif)\n",
    "    print(\"Total {} frames in the gif!\".format(nums))\n",
    "    # convert form RGB to BGR \n",
    "    if len(gif[0].shape)>2:\n",
    "        imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in gif]\n",
    "    else:\n",
    "        imgs = [im for im in gif]\n",
    "    return imgs\n",
    "def get_uvs(imgs,window=15):\n",
    "    uvs = []\n",
    "    for i in range(len(gimgs)-1): \n",
    "        u,v = optical_flow(imgs[i],imgs[i+1],15)\n",
    "        #u,v = np.rot90(u),np.rot90(v)\n",
    "        uvs.append((u,v))\n",
    "    return uvs\n",
    "def draw_moving_field(uvs,imgs,selected):\n",
    "    for r,im in enumerate(imgs[:-2]):\n",
    "        u,v = uvs[r][0],uvs[r][1]\n",
    "        for i,j in selected:\n",
    "            pt = np.array([i,j])\n",
    "            uv = np.array([u[i,j],v[i,j]])*4\n",
    "            pt2 = ((pt + uv)).astype(np.int)\n",
    "            cv2.arrowedLine(imgs[r], tuple(pt), tuple(pt2), (255,0,0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20 frames in the gif!\n"
     ]
    }
   ],
   "source": [
    "f1,t1,s1 = 0,260,10\n",
    "f2,t2,s2 = 0,240,10\n",
    "selected = [(i,50) for i in range(f1,t1,s1)]\n",
    "for j in range(f2,t2,s2):\n",
    "    selected += [(i,j) for i in range(f1,t1,s1)]\n",
    "    \n",
    "imgs = decode_gif_file(\"imgs/rubic.gif\")\n",
    "uvs_rubic = get_uvs(imgs)\n",
    "## For some reason this must be flipped and rotated to work\n",
    "uvs_rubic = [(np.fliplr(np.rot90(u,3)),np.fliplr(np.rot90(v,3))) for u,v in uvs_rubic]\n",
    "cimgs = [cv2.cvtColor(im, cv2.COLOR_GRAY2RGB) for im in imgs]\n",
    "draw_moving_field(uvs_rubic,cimgs,selected)\n",
    "imageio.mimsave('imgs/rubic_rd.gif', cimgs,fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/rubic_rd.gif\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 19 frames in the gif!\n"
     ]
    }
   ],
   "source": [
    "f1,t1,s1 = 0,200,8\n",
    "f2,t2,s2 = 0,200,8\n",
    "selected = [(i,50) for i in range(f1,t1,s1)]\n",
    "for j in range(f2,t2,s2):\n",
    "    selected += [(i,j) for i in range(f1,t1,s1)]\n",
    "    \n",
    "imgs = decode_gif_file(\"imgs/sphere.gif\")\n",
    "gimgs = [gray(im) for im in imgs]\n",
    "uvs_sphere = get_uvs(gimgs)\n",
    "## For some reason this must be flipped and rotated to work\n",
    "uvs_sphere = [(np.fliplr(np.rot90(u,3)),np.fliplr(np.rot90(v,3))) for u,v in uvs_sphere]\n",
    "draw_moving_field(uvs_sphere,imgs,selected)\n",
    "imageio.mimsave('imgs/sphere_rd.gif', imgs,fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/sphere_rd.gif\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "When does the optical flow equation become solvable?<br/>\n",
    "1. $\\lambda_1 \\& \\lambda_2$ are small<br/>\n",
    "2. $\\lambda_1 >> \\lambda_2$<br/>\n",
    "3. $\\lambda_1 << \\lambda_2$<br/>\n",
    "4. <font color=\"green\"> $\\lambda_1$~$\\lambda_2$ both fairly large<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB Version\n",
    "\n",
    "- One method: pretend the pixel's neighbors have the **same** $(u,v)$\n",
    "\n",
    "$$0 = I_t(p_i)[0,1,2]+\\nabla I(p_i)[0,1,2]\\cdot[u\\,v]$$\n",
    "<br/>\n",
    "$$A_{75\\times2} \\times d_{2\\times1} = b_{75\\times1}$$\n",
    "<br/>\n",
    "$$\\begin{bmatrix}I_x(p_1)[0] & I_y(p_1)[0]\\\\I_x(p_1)[1] & I_y(p_1)[1]\\\\I_x(p_1)[2] & I_y(p_1)[2]\\\\.&.\\\\.&.\\\\.&.\\\\I_x(p_25)[0] & I_y(p_25)[0]\\\\I_x(p_25)[0] & I_y(p_25)[0]\\\\I_x(p_25)[1] & I_y(p_25)[2]\\end{bmatrix}\\begin{bmatrix}u\\\\v\\end{bmatrix} = - \\begin{bmatrix}I_t(p_1)[0]\\\\I_t(p_1)[1]\\\\I_t(p_1)[2]\\\\.\\\\.\\\\.\\\\I_t(p_25)[0]\\\\I_t(p_25)[1]\\\\I_t(p_25)[2]\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "*Note that ***RGB*** alone at a pixel is not enough to disambiguate because R,G & B are correlated. Just provides better gradient*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors in Lucas Kanade\n",
    "\n",
    "- The mortion is large (larger than a pixel)- Taylor doesn't hold\n",
    "    - Not-linear: Iterative refinment\n",
    "    - Local minima: corse-to-fine estimation\n",
    "\n",
    "### Not tangent: Iterative Refinement\n",
    "\n",
    "**Iterative Lukas-Kanade Algorithm**<br/>\n",
    "1. Estimate velocity at each pixel by solving Lucas-Kanade equations\n",
    "2. Warp $I_t$ towards $I_{t+1}$ using the estimated flow field\n",
    "    - Use image warping techniques\n",
    "3. Repeat until convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow: Iterative Estimation\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"imgs/L623.png\" width=\"400px\" />\n",
    "        <img src=\"imgs/L624.png\" width=\"400px\" />\n",
    "        <img src=\"imgs/L625.png\" width=\"400px\" />\n",
    "    <img src=\"imgs/L626.png\" width=\"400px\" />\n",
    "    <center><figcaption>Fig.13 </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "## Implementation Issues\n",
    "\n",
    "- Warping is not easy (ensure that errors in warping are smaller than the estimate refinement)- but it is in Matlab\n",
    "- Ofen useful to low-pass filter the images before motion estimation (for better derivative estimation, and linear approximations to image intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
